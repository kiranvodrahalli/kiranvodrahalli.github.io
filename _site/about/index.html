<!DOCTYPE html>
<html lang="en">

  <head>

   
   <!--
     redirect users to the https version of the website.
     but: only check when on the production domain, as set in _config.yml.
    -->
   <script type="text/javascript">
     var enforce = "kiranvodrahalli.github.io";
     if ((enforce == window.location.host) && (window.location.protocol != "https:"))
       window.location = window.location.toString().replace(/^http:/, "https:");
   </script>
   

  <!-- EB Garamond Font --> 
  <link href='https://fonts.googleapis.com/css?family=EB+Garamond:400,100,100italic,300,300italic,400italic,500,500italic,700,700italic,900italic,900' rel='stylesheet' type='text/css'>
  <meta charset="UTF-8">
  <link href="https://gmpg.org/xfn/11" rel="profile"> 

  <!-- Roboto Font --> 
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,400italic,500,500italic,700,700italic,900italic,900" rel='stylesheet' type='text/css'>
  <meta charset="UTF-8">
  <link href="https://gmpg.org/xfn/11" rel="profile">

    <!-- Gelasio Font --> 
  <link href="https://fonts.googleapis.com/css?family=Gelasio:400,100,100italic,300,300italic,400italic,500,500italic,700,700italic,900italic,900" rel='stylesheet' type='text/css'>
  <meta charset="UTF-8">
  <link href="https://gmpg.org/xfn/11" rel="profile">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      about || exponentially surprised
    
  </title>

  <!-- MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ 
      TeX: { 
        extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "bbox.js", "color.js", "enclose.js"] 
      },
      unicode: {
        fonts: "STIXGeneral, 'Arial Unicode MS'"
      }
    });
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>



  <!-- CSS -->
  <link rel="stylesheet" href="/styles.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/public/favicomatic/apple-touch-icon-57x57.png" />
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/public/favicomatic/apple-touch-icon-114x114.png" />
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/public/favicomatic/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicomatic/apple-touch-icon-144x144.png" />
  <link rel="apple-touch-icon-precomposed" sizes="60x60" href="/public/favicomatic/apple-touch-icon-60x60.png" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/public/favicomatic/apple-touch-icon-120x120.png" />
  <link rel="apple-touch-icon-precomposed" sizes="76x76" href="/public/favicomatic/apple-touch-icon-76x76.png" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/public/favicomatic/apple-touch-icon-152x152.png" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-196x196.png" sizes="196x196" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-96x96.png" sizes="96x96" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-32x32.png" sizes="32x32" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-16x16.png" sizes="16x16" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-128.png" sizes="128x128" />
  <link rel="shortcut icon" type="image/png" href="/public/favicomatic/favicon.ico" />

  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="exponentially surprised" href="/atom.xml">
</head>


  <body>

    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <font size="8">Kiran Vodrahalli</font> 
        </h3>
        <!-- is <a href="/" title="Home">||exponentially surprised||</a>
          <small>[2<sup>H</sup> Theory Blog]</small> 
        -->
        <a href="/about" title="about">{about}</a>
        <a href="/research" title="research">{research}</a>
        <a href="/talks" title="talks">{talks}</a>
        <a href="/blog" title="blog">{blog}</a>
        <a href="/archive" title="archive">{archive}</a>
        <a href="/teaching" title="teaching">{teaching}</a>
        <a href="/notes" title="notes">{notes}</a>
        <a href="/links" title="links">{links}</a>
        <a href="/press" title="press">{press}</a>
        <!-- <div align="left">
          <img src="/professional_headshot.jpg" alt="kiran_pic" height="400">
        </div> -->
      </header>

      <main>
        <article class="page">
  <h1 class="page-title">about</h1>
  <!-- example of the message class
<p class="message">
  My name is Kiran Vodrahalli. 
</p>
-->

<!-- add picture -->
<p><img src="/public/kiran-profile.jpg" alt="image-title-here" class="img-responsive element" height="30%" width="30%" /></p>

<h3 id="currently">Currently…</h3>

<p>I am a research scientist at Google Brain, where I study resource-efficient machine learning in many contexts, with a current focus on long-range sequence modeling and time series, and particularly the role of memory in these problems. I also work on questions at the intersection of multi-agent learning, strategic learning in games, incentives for data collection and collaborative machine learning, and algorithmic game theory, and am interested in associated problems in algorithms and optimization theory. I have also worked on applications of machine learning in several fields, including neuroscience, natural language understanding, economics, and robotics.</p>

<p>Please <a href="mailto:kiran.vodrahalli@columbia.edu">email me</a> if you think we might have some overlapping interests. I am always happy to chat and possibly collaborate!</p>

<p>Current Affiliations:</p>
<ul>
  <li><a href="https://research.google/teams/brain/">Google Brain Research</a></li>
</ul>

<p>Other Info:</p>
<ul>
  <li><a href="mailto:kiran.vodrahalli@columbia.edu">Email</a></li>
  <li><a href="https://twitter.com/kiranvodrahalli">Twitter</a></li>
  <li><a href="https://github.com/kiranvodrahalli">Github</a></li>
  <li><a href="https://www.goodreads.com/review/list/6132224">GoodReads</a></li>
  <li><a href="https://www.linkedin.com/in/kiranvodrahalli/">LinkedIn</a></li>
  <li><a href="https://www.quora.com/profile/Kiran-Vodrahalli">Quora</a></li>
</ul>

<p>For more details, either check out this website or see my <a href="/about/cv.pdf" title="cv"> [CV] </a>.</p>

<hr />

<h3 id="news">News</h3>

<p>I graduated with my Ph.D. from the Computer Science Department at Columbia University in June 2022! I will be joining Google Brain as a Research Scientist in Fall 2022.</p>

<!-- TODO FOR LATER?
I am looking to mentor ... people in machine learning. 
-->

<!--
I am currently looking for postdoctoral and research positions starting in Fall 2022; I am interested in both applied and theoretical (or some mix thereof) research in machine learning (see my <a href="/about/cv.pdf" title="cv"> [CV] </a> ). 
I am interested in focusing on one of the following topics in my next role: 

#### Memory Bounded and Resource Efficient Learning
* memory-bounded and resource-efficiency aspects of
	* bandit/reinforcement learning
	* language modeling and understanding
	* representation learning

#### Learning and Algorithmic Game Theory (see [The Platform Design Problem](https://arxiv.org/abs/2009.06117))
* game theoretic analysis of bi-level environment design \\
and applications to internet economics and reinforcement learning
* manipulation of learning agents and strategic behavior of learning agents in response to manipulation
* data collection mechanisms and privacy/fairness considerations
* new concepts of equilibria in learning in games and multi-agent learning settings
* incentives and strategic behavior in machine learning

#### Fusing Logic and Learning (see [Learning and Planning with Logical Automata](https://link.springer.com/article/10.1007/s10514-021-09993-6))
* outfitting large blackbox generative models/policies with interpretable controls
* interpretable machine learning via interactive learning
* learning the "grammar" of action sequences (ex: learning the rules of the road)

-->

<hr />

<!--
### Research Interests


My primary area of research is theoretical computer science: in particular, provably resource-efficient algorithms for fitting statistical models in various settings ("algorithmic statistics", "foundations of machine learning", "learning theory", etc.). Some of my work in this direction has skewed in the direction of giving computationally efficient, low sample complexity algorithms for learning functions with sparse descriptions. I am also interested in algorithms and optimization theory. I have also worked on applications of machine learning in several fields, including neuroscience, natural language understanding, economics, and robotics. 

-->

<!--
Currently, I am particularly focused on designing algorithms and proving lower bounds for memory-bounded learning and optimization problems. I am also working on applying ideas from machine learning (online learning, learning in games, reinforcement learning) and bi-level optimization to understand computational and statistical issues associated with the economics of the online firm, as well as associated privacy, ethics, and fairness concerns (see my recent paper [The Platform Design Problem](https://arxiv.org/abs/2009.06117)). 


---

-->

<h3 id="previously">Previously…</h3>

<p>From Fall 2017 to Summer 2022, I was a <a href="http://www.cs.columbia.edu/">Computer Science</a> Ph.D. student at Columbia University, focusing on theoretical computer science (<a href="http://theory.cs.columbia.edu/">Columbia CS Theory</a>), with particular interest in machine learning (<a href="https://ml.cs.columbia.edu/">Columbia Machine Learning</a>), algorithms, and statistics. My thesis topic was resource-efficient machine learning. I was fortunate to be advised by <a href="http://www.cs.columbia.edu/~djhsu/">Professor Daniel Hsu</a> and <a href="http://www.mit.edu/~andoni/">Professor Alex Andoni</a>. I was supported by an NSF Graduate Research Fellowship during my Ph.D.</p>

<p>In Fall 2021 and early Spring 2022 I was also a part-time student researcher at Google Brain.</p>

<p>In Summer 2021 I was a research intern at Google Brain, where I worked on principled resource-efficient methods for training deep neural networks.</p>

<p>In Summer 2019 I visited the Simons Institute at Berkeley for the <a href="https://simons.berkeley.edu/programs/dl2019">Foundations of Deep Learning</a> program.</p>

<p>I graduated from <a href="https://www.princeton.edu">Princeton</a> with an A.B. Mathematics degree with honors in 2016 and an M.S.E. in Computer Science in 2017, where I was lucky to have <a href="http://www.cs.princeton.edu/~arora/">Professor Sanjeev Arora</a> and <a href="https://psych.princeton.edu/person/kenneth-norman">Professor Ken Norman</a> as thesis advisors. I was a member of Sanjeev Arora’s <a href="http://unsupervised.cs.princeton.edu/members.html">Unsupervised Learning Group</a>, where I studied provable methods for machine learning (also a part of <a href="http://nlp.cs.princeton.edu/">NLP @ Princeton</a> and <a href="https://mltheory.cs.princeton.edu/people/">ML Theory @Princeton</a>), in particular focusing on natural language understanding. I was also a member of Ken Norman’s <a href="http://compmem.princeton.edu/lab-people/">Computational Memory Lab</a> at the <a href="http://pni.princeton.edu">Princeton Neuroscience Institute</a>, where I applied machine learning to fMRI analysis methods.</p>


</article> 



      </main>

      <footer class="footer">
        <small>
          &copy; <time datetime="2022-08-30T20:38:36-04:00">2022</time>. All rights reserved.
        </small>
      </footer>
    </div>

  </body>
</html>


<!DOCTYPE html>
<html lang="en">

  <head>

   
   <!--
     redirect users to the https version of the website.
     but: only check when on the production domain, as set in _config.yml.
    -->
   <script type="text/javascript">
     var enforce = "kiranvodrahalli.github.io";
     if ((enforce == window.location.host) && (window.location.protocol != "https:"))
       window.location = window.location.toString().replace(/^http:/, "https:");
   </script>
   

  <!-- EB Garamond Font --> 
  <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
  <meta charset="UTF-8">
  <link href="https://gmpg.org/xfn/11" rel="profile">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      projects || representing things
    
  </title>

  <!-- MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ 
      TeX: { 
        extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "bbox.js", "color.js", "enclose.js"] 
      },
      unicode: {
        fonts: "STIXGeneral, 'Arial Unicode MS'"
      }
    });
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>



  <!-- CSS -->
  <link rel="stylesheet" href="/styles.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/public/favicomatic/apple-touch-icon-57x57.png" />
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/public/favicomatic/apple-touch-icon-114x114.png" />
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/public/favicomatic/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicomatic/apple-touch-icon-144x144.png" />
  <link rel="apple-touch-icon-precomposed" sizes="60x60" href="/public/favicomatic/apple-touch-icon-60x60.png" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/public/favicomatic/apple-touch-icon-120x120.png" />
  <link rel="apple-touch-icon-precomposed" sizes="76x76" href="/public/favicomatic/apple-touch-icon-76x76.png" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/public/favicomatic/apple-touch-icon-152x152.png" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-196x196.png" sizes="196x196" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-96x96.png" sizes="96x96" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-32x32.png" sizes="32x32" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-16x16.png" sizes="16x16" />
  <link rel="icon" type="image/png" href="/public/favicomatic/favicon-128.png" sizes="128x128" />
  <link rel="shortcut icon" type="image/png" href="/public/favicomatic/favicon.ico" />

  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="representing things" href="/atom.xml">
</head>


  <body>

    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home">||representing things||</a>
          <small>[kiran's theoryblog]</small>
        </h3>
        <a href="/about" title="about">{about}</a>
        <a href="/projects" title="projects">{projects}</a>
        <a href="/notes" title="notes">{notes}</a>
        <a href="/contact" title="contact">{contact}</a>
      </header>

      <main>
        <article class="page">
  <h1 class="page-title">projects</h1>
  <!-- example of the message class
<p class="message">
  My name is Kiran Vodrahalli. 
</p>
-->

<h2>Class Projects</h2>

<h3>Solving Word Analogies With Convex Optimization</h3>

<p>My final project for <a href= "http://www.cs.princeton.edu/courses/archive/spring15/cos511/" title= "cos511"> COS 511: Theoretical Machine Learning</a> investigated convex loss functions for learning word vectors to solve word analogy problems. Word analogies are of the form king:man :: queen:woman. Given three of the four words, the task is to correctly identify the fourth. Traditionally, this problem is approached in the unsupervised setting and texts are used to learn which words are most relevant. Word vectors, word representations in high-dimensional real space, are often used (particularly in the past few years) as a solution to the analogy problem via dot-product queries, an approach which has recently been validated by <a href= "http://arxiv.org/abs/1502.03520" title= "random_walks_semantic_space"> [Arora et al (2015)]</a>. I formulated a convex loss with which to train word vectors that in principle keeps the spirit of the dot product query intuition, implemented AdaGrad <a href= "http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" title= "AdaGrad"> [Duchi et al (2011)]</a>, and trained on word pairs. This project is a work in progress, as I am continuing to validate experimental results. Stay tuned for an ArXiv link!</p>

<h3>Analyzing Paired EEG-fMRI Data <a href= "/projects/cos513/" title= "cos513"> [COS 513 Blog] </a></h3>

<p>This work is in progress, joint with Lydia Liu (Princeton ORFE &#39;17) and Niranjani Prasad (Grad Student, Princeton CS Department). We are analyzing paired EEG-fMRI time series data as a project for <a href= "http://www.cs.princeton.edu/~bee/courses/cos513.html" title= "cos513web"> COS 513: Foundations of Probabilistic Modeling</a>, taught by Professor Barbara Engelhardt.</p>

<h3>Comparing Hebbian Semantic Vectors Across Language <a href= "/projects/neu330paper.pdf" title= "neu330"> [pdf] </a></h3>

<p>My final project for NEU 330, Connectionist Models, focused on building Hebbian neural network word vector models for parallel corpora, with the purpose of evaluating the word vectors based on how similarly the word vectors for translation pairs behaved in their respective corpora. The principle I held throughout the project was simply that changing language should essentially not effect the representation of a word/concept in a high-dimensional vector space. I both proposed methods of evaluation and made use of previously used metrics to evaluate the 9 models considered. The texts used to form the word vectors were Harry Potter and The Philosopher&#39;s Stone and its French counterpart. </p>

<h3>A Presentation on Deep Learning for Natural Language Processing <a href="/projects/598b_nlp_deep_learning.pdf" title="598c_nlp"> [slides] </a></h3>

<p>My first presentation for the Spring 2015 offering of <a href = "http://vision.princeton.edu/courses/COS598/2015sp/" title="COS 598B website"> COS 598B </a>, an advanced topics class on deep learning taught by Professor Jianxiong Xiao. I overviewed various approaches to solving NLP problems with deep learning approaches, with a stronger focus on language models and word vector embeddings. </p>

<h3>A Presentation on Image Captioning <a href="/projects/598b_img_captions.pdf" title="598c_img_cap"> [slides] </a></h3>

<p>My second presentation for <a href = "http://vision.princeton.edu/courses/COS598/2015sp/" title="COS 598B website"> COS 598B </a> focused on the image captioning problem, and deep learning approaches to solving it.</p>

<h3>Estimating Trending Twitter Topics With Count-Min Sketch <a href= "/projects/cos521paper.pdf" title= "cos521"> [pdf] </a></h3>

<p>My final project for <a href= "http://www.cs.princeton.edu/courses/archive/fall14/cos521/" title= "cos521"> COS 521: Advanced Algorithms</a> was joint with Evan Miller (Princeton COS &#39;16) and Albert Lee (Princeton COS &#39;16). We attempted to solve the following problem: Given a time series of Twitter data, can we infer current trending topics on Twitter while appropriately discounting past tweets using a sketch-based approach? We tweaked the Hokusai data structure <a href= "http://www.auai.org/uai2012/papers/231.pdf" title= "Hokusai"> [Matusevych et al 2012]</a> and implemented it, then ran experiments on Twitter data. </p>

<h2>Independent Work</h2>

<h3>A Presentation on Expander Graphs <a href="/projects/jsem2015paper.pdf" title="jsem"> [pdf] </a></h3>

<p>My junior seminar, taught by Professor Zeev Dvir in the spring of 2015, was on Point-Line Incidence Theorems. My presentation focused on a tangent subject, expander graphs. </p>

<h3>Noun Compounds in Semantic Quad-Space <a href="/projects/iw2014paper.pdf" title= "iw2014"> [pdf] </a></h3>

<p>My independent work in the fall of 2014 was advised by Christiane Fellbaum. I looked at several approaches to analyzing the similarity of noun compounds and build a vector space model of noun compounds, inspired by the papers of Turney <a href= "http://arxiv.org/abs/1309.4035" title="Domain_and_function"> [Turney 2013]</a> and Fyshe <a href= "http://www.aclweb.org/anthology/W13-3510" title="fyshe_paper"> [Fyshe et al 2013] </a>. I extended the dual-space model of Turney to a quad space model and ran it on two large corpora, <a href= "http://corpus.byu.edu/coca/" title="coca"> COCA </a>  and <a href= "http://corpus.byu.edu/glowbe/" title="glowbe"> GloWbE </a>. I then evaluated the results by comparing to a ground truth provided by Mechanical Turk workers.  </p>

<!-- # Coding Projects -->

</article> 



      </main>

      <footer class="footer">
        <small>
          &copy; <time datetime="2015-09-30T14:30:11-04:00">2015</time>. All rights reserved.
        </small>
      </footer>
    </div>

  </body>
</html>


---
layout: page
title: about
---


<!-- example of the message class
<p class="message">
  My name is Kiran Vodrahalli. 
</p>
-->

I am currently a Ph.D. student at Columbia University, focusing on theoretical computer science, with particular interest in machine learning, algorithms, and statistics. I am extremely fortunate to be advised by [Daniel Hsu](http://www.cs.columbia.edu/~djhsu/) and [Alex Andoni](http://www.mit.edu/~andoni/). I am affiliated with the [Computer Science department](http://www.cs.columbia.edu/), particularly the [Theory group](http://www.cs.columbia.edu/theory/) and the [Machine learning group](http://www.cs.columbia.edu/areas/machine/), as well as the [Data Science Institute](http://datascience.columbia.edu/).

My primary area of theoretical research is provable algorithms and models for computational inference and optimization. A lot of my interests can be summarized by the phrase ``Non-worst-case analysis for machine learning algorithms''. I am particularly interested in the following broad areas: 

* Theoretical frameworks for unsupervised learning which allow one to give guarantees on performance

* Interactive learning applied to evaluation methodology and learning problem formulation

* Generalization bounds for models which do not currently have completely satisfactory guarantees (read my lips d-e-e-p l-e-a-r-n-i-n-g). In particular, I am interested in data-dependent bounds on the sample complexity.

* Non-convex optimization with guarantees

* Better characterization of easy-to-check data properties which ensure good statistical/computational efficiency. Also, developing data collection methods to ensure such properties exist in datasets. 

* Highly-structured learning: learning rules and logic in reinforcement learning settings. I am also interested in models which blend rigid structure (like logic) with statistical models. 

* Understanding sequence models: Bridging the gap between HMMs and RNNs. 

* Identifying and learning over low-dimensional structures (of all sorts, but recently I have focused on various notions of sparsity). In particular, the goal is to give algorithms which achieve sample complexities and computational complexities which depend on the ``low-dimensional'' part of the structure, rather than the (potentially high-dimensional) ambient space.

* Identifying failure modes of learning algorithms and models

* Theoretically justifying generation models

In applications, I am particularly interested in natural language understanding and neuroscience. 

Previously, I graduated from [Princeton](https://www.princeton.edu) with an A.B. Mathematics degree with honors in 2016 and an M.S.E. in Computer Science in 2017. I was a member of Sanjeev Arora's [Unsupervised Learning Group](http://unsupervised.cs.princeton.edu/members.html), where I studied provable methods for machine learning. One applications of particular interest was natural language understanding. I was also a member of Ken Norman's [Computational Memory Lab](http://compmem.princeton.edu/lab-people/) at the [Princeton Neuroscience Institute](http://pni.princeton.edu), where I applied machine learning to fMRI analysis methods. 


For more details, either check out this website or see the essentials summarized in my <a href="{{ site.baseurl }}/about/cv.pdf" title="cv"> curriculum vitae </a>.
